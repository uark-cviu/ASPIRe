<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>ASPIRe</title>
    <link rel="icon" type="image/x-icon" href="static/images/UA_Logo.png">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-825ET94Y3E"></script>
    <script>window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-825ET94Y3E');</script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in <br> Video Understanding</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"> <a href="https://uark-cviu.github.io/" target="_blank">https://uark-cviu.github.io/</a></span>
                        </div>
                        <div class="is-size-5 publication-authors">
                          <span class="author-block"> <a href="https://scholar.google.com/citations?user=ty0Njf0AAAAJ&hl=vi&authuser=1" target="_blank">Trong-Thuan Nguyen</a>, </span>
                            <span class="author-block"> <a href="https://pha-nguyen.github.io/" target="_blank">Pha Nguyen</a>, </span>
                            <span class="author-block"> <a href="https://scholar.google.com/citations?user=JPAl8-gAAAAJ" target="_blank">Khoa Luu</a></span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">University of Arkansas &nbsp; </span>
                        </div>
                        <div class="column has-text-centered">
                            <!-- <h2 class="title">NeurIPS 2023</h2> -->
                            <div class="publication-links">
                                <span class="link-block"> <a href="./static/pdfs/main_paper.pdf" target="_blank" class="external-link button is-normal is-rounded"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Paper</span> </a> </span>
                                <span class="link-block"> <a href="./static/pdfs/supp.pdf" target="_self" class="external-link button is-normal is-rounded"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Supplementary</span> </a> </span>
                                <span class="link-block"> <a href="#annotations" target="_self" class="external-link button is-normal is-rounded"> <span class="icon"> <i class="fas fa-download"></i> </span> <span>Dataset</span> </a> </span>
                                <!-- <span class="link-block"> <a href="https://github.com/uark-cviu/Type-to-Track" target="_blank" class="external-link button is-normal is-rounded"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Devkit</span> </a> </span> -->
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="hero teaser">
      <div class="container is-max-desktop">
          <div class="hero-body">
              <div id="results-carousel" class="carousel results-carousel">
                  <div class="item"> <video poster="static/images/basketball.jpg" autoplay muted loop playsinline
                          height="100%">
                          <source src="static/videos/demo-basketball.mp4" type="video/mp4">
                      </video> </div>
                  <div class="item"> <video poster="static/images/umbrella.jpg" autoplay muted loop playsinline
                          height="100%">
                          <source src="static/videos/demo-umbrella.mp4" type="video/mp4">
                      </video> </div>
                  <div class="item"> <video poster="static/images/film.jpg" autoplay muted loop playsinline
                        height="100%">
                        <source src="static/videos/demo-film.mp4" type="video/mp4">
                    </video> </div>
              </div>
              <div class="captions has-text-justified">Qualitative results of <i>position</i>, <i>interaction</i>, and <i>relation</i> 
                from scene graphs generated from the <b><i>Hierarchical Interlacement Graph (HIG)</i></b> model on our <b><i>ASPIRe</i></b> dataset.</div>
          </div>
      </div>
  </section>
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>Visual interactivity understanding within visual scenes presents a significant challenge in computer vision. 
                          Existing methods focus on complex interactivities while leveraging a simple relationship model. 
                          These methods, however, struggle with a diversity of appearance, situation, position, interaction, and relation in videos. 
                          This limitation hinders the ability to fully comprehend the interplay within the complex visual dynamics of subjects. 
                          In this paper, we delve into interactivities understanding within visual content by deriving scene graph representations from dense interactivities among humans and objects. 
                          To achieve this goal, we first present a new dataset containing <i>Appearance-Situation-Position-Interaction-Relation</i> predicates, named <i>ASPIRe</i>, 
                          offering an extensive collection of videos marked by a wide range of interactivities. Then, we propose a new approach named <i>Hierarchical Interlacement Graph (HIG)</i>, 
                          which leverages a unified layer and graph within a hierarchical structure to provide deep insights into scene changes across five distinct tasks. 
                          Our approach demonstrates superior performance to other methods through extensive experiments conducted in various scenarios.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section hero is-small">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Introduction</h2>
                    <div class="content has-text-justified">
                        <p><i>We introduce the new <i>ASPIRe</i> dataset to Visual Interactivity Understanding.
                          The diversity of the <i>ASPIRe</i> dataset is showcased through its wide range of scenes and settings, distributed in seven scenarios.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <table class="title is-1 publication-title">
                        <thead>
                            <tr>
                                <th>1,488 videos</th>
                                <th>167,751 annotations</th>
                                <th>4,549 interactivities</th>
                            </tr>
                        </thead>
                    </table>
                </div>
            </div>
        </div>
    </section>
    <section class="section hero is-small">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <div class="content has-text-justified">
                        <p>Examples of annotations found on the <i>ASPIRe</i> dataset: </p>
                    </div>
                </div>
            </div> <video poster="static/images/basketball.jpg" autoplay muted loop playsinline height="100%">
                <source src="static/videos/teaser-aspire.mp4" type="video/mp4">
            </video>
        </div>
    </section>
    <section class="section hero is-small" id="annotations">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3 has-text-centered">Annotations</h2>
                    <div class="column">
                        <h2 class="title is-5"> v1.0: </h2>
                        <div class="content">
                            <ul>
                                <li>The category <span style="background-color: #dfdfdf"><b>name</b></span>, <span
                                        style="background-color: #c0bffe"><b>bbox</b></span>,  <span
                                        style="background-color: #f9dbc3"><b>segmentation</b></span> and <span
                                        style="background-color: #c7ffbf"><b>track_id</b></span> compatible with that <a href="https://taodataset.org/">TAO</a> dataset.</li>
                                    <ul>
                                        <li>Training set: <a href="./annotations/v1.0/train.json" target="_blank">[Train annotations]</a></li>
                                        <li>Testing set: <a href="./annotations/v1.0/test.json" target="_blank">[Test annotations]</a></li>
                                    </ul>
                            </ul>
                        </div>
                        <div class="content"> The annotations of <i>ASPIRe</i> and the original source videos are released under a <a
                                href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank">CC BY-NC-SA
                                3.0</a> license per their creators. See <a href="https://motchallenge.net/"
                                target="_blank">motchallenge.net</a> for details. </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section hero is-small is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column">
                    <h2 class="title is-3 has-text-centered">Statistics</h2> <img src="static/images/Statistics.png"
                        style="display: block; margin-left: auto; margin-right: auto" />
                </div>
            </div>
        </div>
    </section>
    <section class="section hero is-small is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column">
                    <h2 class="title is-3 has-text-centered">Comparison</h2> <img src="static/images/ASPIRe.png"
                        style="display: block; margin-left: auto; margin-right: auto" />
                    <!-- <div id="results-carousel" class="carousel results-carousel"> <div class="item"> <img src="static/images/GroOT.png" style="display: block; margin-left: auto; margin-right: auto" /> </div> <div class="item"> <img src="static/images/MENDER.png" style="display: block; margin-left: auto; margin-right: auto" /> </div> </div> -->
                </div>
            </div>
        </div>
    </section>
    <!-- <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3 has-text-centered">BibTeX</h2>
                    <pre><code>@article{nguyen2023type,
    title        = {Type-to-Track: Retrieve Any Object via Prompt-based Tracking},
    author       = {Nguyen, Pha and Quach, Kha Gia and Kitani, Kris and Luu, Khoa},  
    journal      = {Advances in Neural Information Processing Systems},
    year         = 2023
}</code></pre>
                </div>
            </div>
        </div>
    </section> -->
    <!-- <section class="section">
        <div class="container is-max-desktop content">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3 has-text-centered">Prior Works</h2>
                    <div class="content has-text-justified">
                        <p>[1] Kha Gia Quach, Pha Nguyen, Huu Le, Thanh-Dat Truong, Chi Nhan Duong, Minh-Triet Tran, and Khoa Luu. "<b>DyGLIP: A dynamic graph model with link prediction for accurate multi-camera multiple object tracking.</b>" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Quach_DyGLIP_A_Dynamic_Graph_Model_With_Link_Prediction_for_Accurate_CVPR_2021_paper.pdf">paper</a>, <a href="https://github.com/uark-cviu/DyGLIP">code</a>]</p>
                        <p>[2] Pha Nguyen, Kha Gia Quach, Chi Nhan Duong, Ngan Le, Xuan-Bac Nguyen, and Khoa Luu. "<b>Multi-camera multiple 3d object tracking on the move for autonomous vehicles.</b>" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2022. [<a href="https://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Nguyen_Multi-Camera_Multiple_3D_Object_Tracking_on_the_Move_for_Autonomous_CVPRW_2022_paper.pdf">paper</a>]</p>
                        <p>[3] Pha Nguyen, Kha Gia Quach, Chi Nhan Duong, Son Lam Phung, Ngan Le, and Khoa Luu. "<b>Multi-Camera Multi-Object Tracking on the Move via Single-Stage Global Association Approach.</b>" Under review. [<a href="https://browse.arxiv.org/pdf/2211.09663.pdf">paper</a>]</p>
                        <p>[4] Pha Nguyen, Kha Gia Quach, John Gauch, Samee U. Khan, Bhiksha Raj, and Khoa Luu. "<b>UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation.</b>" Under review. [<a href="https://arxiv.org/pdf/2306.09613.pdf">paper</a>]</p>
                        <p>[5] Kha Gia Quach, Huu Le, Pha Nguyen, Chi Nhan Duong, Tien Dai Bui, and Khoa Luu. "<b>Depth Perspective-aware Multiple Object Tracking.</b>" Under review. [<a href="https://arxiv.org/pdf/2207.04551.pdf">paper</a>]</p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container">
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3 has-text-centered">Sponsors</h2>
                </div>
            </div>
            <table>
                <tr>
                    <th style="text-align: center; vertical-align: middle;"><img src="./static/images/sponsors/NSF.png" alt="NSF"></th>
                    <th style="text-align: center; vertical-align: middle;"><img src="./static/images/sponsors/DART.png" alt="DART"></th>
                    <th style="text-align: center; vertical-align: middle;"><img src="./static/images/sponsors/google_research.png" alt="Google"></th>
                    <th style="text-align: center; vertical-align: middle;"><img src="./static/images/sponsors/ahpcc.png" alt="AHPCC"></th>
                </tr>
            </table>
        </div>
    </section> -->
    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content is-small">
                        <p>This page was built using the <a
                                href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                target="_blank">Academic Project Page Template</a>. <br> This website is licensed under
                            a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                                target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>

</html>